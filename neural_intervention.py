import os
import torch
import hashlib
import argparse
import numpy as np
import matplotlib.pyplot as plt
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.nn import functional as F
import copy
from typing import Dict, List, Tuple, Optional, Union


class NeuralInterventionController:
    """ç¥ç»å¹²é¢„æ§åˆ¶å™¨ï¼Œç”¨äºå¯¹è¯­è¨€æ¨¡å‹çš„ç‰¹å®šå±‚å’Œç‰¹å®šç»´åº¦è¿›è¡Œå¹²é¢„"""
    
    def __init__(self, model_name_or_path: str, device: str = "cuda", gpu_id: int = 0):
        """
        åˆå§‹åŒ–å¹²é¢„æ§åˆ¶å™¨
        
        Args:
            model_name_or_path: æ¨¡å‹è·¯å¾„æˆ–åç§°
            device: è®¾å¤‡ç±»å‹ï¼ˆcudaæˆ–cpuï¼‰
            gpu_id: GPUè®¾å¤‡IDï¼ˆå½“deviceä¸ºcudaæ—¶ç”Ÿæ•ˆï¼‰
        """
        # è®¾ç½®å…·ä½“çš„è®¾å¤‡
        if device == "cuda" and torch.cuda.is_available():
            self.device = f"cuda:{gpu_id}"
            # è®¾ç½®é»˜è®¤GPUè®¾å¤‡
            torch.cuda.set_device(gpu_id)
            print(f"ä½¿ç”¨GPUè®¾å¤‡: {self.device}")
            print(f"GPUè®¾å¤‡åç§°: {torch.cuda.get_device_name(gpu_id)}")
            print(f"GPUæ€»å†…å­˜: {torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3:.2f} GB")
        else:
            self.device = "cpu"
            print(f"ä½¿ç”¨CPUè®¾å¤‡")
        
        self.model_name_or_path = model_name_or_path
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name_or_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path, 
            torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map=self.device
        ).to(self.device)
        
        # è®¾ç½®padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        
        # å­˜å‚¨æ¿€æ´»å€¼çš„å­—å…¸
        self.activations = {}
        # å­˜å‚¨hookçš„åˆ—è¡¨
        self.hooks = []
        # å¹²é¢„é…ç½®
        self.interventions = {}
        
        print(f"æ¨¡å‹å±‚æ•°: {len(self.model.model.layers)}")
        print(f"éšè—çŠ¶æ€ç»´åº¦: {self.model.config.hidden_size}")
    
    def register_activation_hook(self, layer_idx: int):
        """æ³¨å†Œæ¿€æ´»å€¼è·å–é’©å­"""
        def hook_fn(module, input, output):
            # output[0] æ˜¯hidden states
            self.activations[f'layer_{layer_idx}'] = output[0].detach().clone()
        
        # è·å–æŒ‡å®šå±‚
        layer = self.model.model.layers[layer_idx]
        hook = layer.register_forward_hook(hook_fn)
        self.hooks.append(hook)
        return hook
    
    def register_intervention_hook(self, layer_idx: int, intervention_fn):
        """æ³¨å†Œå¹²é¢„é’©å­"""
        def hook_fn(module, input, output):
            # åº”ç”¨å¹²é¢„å‡½æ•°ï¼Œä¼ å…¥ä½ç½®ä¿¡æ¯
            modified_output = intervention_fn(output[0])
            # è¿”å›ä¿®æ”¹åçš„è¾“å‡ºï¼Œä¿æŒåŸå§‹å…ƒç»„ç»“æ„
            return (modified_output,) + output[1:] if len(output) > 1 else (modified_output,)
        
        layer = self.model.model.layers[layer_idx]
        hook = layer.register_forward_hook(hook_fn)
        self.hooks.append(hook)
        return hook
    
    def clear_hooks(self):
        """æ¸…é™¤æ‰€æœ‰é’©å­"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        self.activations.clear()
    
    def set_dimension_intervention(self, layer_idx: int, dimensions: List[int], 
                                 intervention_type: str = "gaussian_replace", 
                                 intervention_value: float = 0.0,
                                 scale_factor: float = 1.0,
                                 gaussian_mean: float = 0.0,
                                 gaussian_std: float = 1.0):
        """
        è®¾ç½®ç‰¹å®šç»´åº¦çš„å¹²é¢„
        
        Args:
            layer_idx: ç›®æ ‡å±‚ç´¢å¼•
            dimensions: è¦å¹²é¢„çš„ç»´åº¦åˆ—è¡¨
            intervention_type: å¹²é¢„ç±»å‹ ("gaussian_replace", "gaussian_noise", "zero", "constant", "scale", "invert")
            intervention_value: å¹²é¢„å€¼ï¼ˆå¯¹äºconstantç±»å‹ï¼‰
            scale_factor: ç¼©æ”¾å› å­ï¼ˆå¯¹äºscaleç±»å‹ï¼‰
            gaussian_mean: é«˜æ–¯åˆ†å¸ƒå‡å€¼ï¼ˆå¯¹äºgaussian_replaceç±»å‹ï¼‰
            gaussian_std: é«˜æ–¯åˆ†å¸ƒæ ‡å‡†å·®ï¼ˆå¯¹äºgaussian_replaceç±»å‹ï¼‰
        """
        def intervention_fn(hidden_states):
            """å¹²é¢„å‡½æ•°ï¼Œå¯¹æ‰€æœ‰tokenè¿›è¡Œå¹²é¢„"""
            modified_states = hidden_states.clone()
            seq_len = hidden_states.shape[1]
            
            # ç®€åŒ–é€»è¾‘ï¼šå¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œå¹²é¢„
            start_pos = 0
            end_pos = seq_len
            
            # å‘é‡åŒ–æ“ä½œï¼šä¸€æ¬¡å¤„ç†æ‰€æœ‰æŒ‡å®šç»´åº¦
            if intervention_type == "gaussian_replace":
                # ç”¨é«˜æ–¯éšæœºåˆ†å¸ƒå®Œå…¨æ›¿ä»£æŒ‡å®šç»´åº¦
                shape = (modified_states.shape[0], end_pos - start_pos, len(dimensions))
                gaussian_values = torch.normal(
                    mean=gaussian_mean, 
                    std=gaussian_std, 
                    size=shape, 
                    device=modified_states.device,
                    dtype=modified_states.dtype
                )
                modified_states[:, start_pos:end_pos, dimensions] = gaussian_values
                        
            elif intervention_type == "gaussian_noise":
                # åœ¨åŸæœ‰æ¿€æ´»å€¼åŸºç¡€ä¸Šæ·»åŠ é«˜æ–¯å™ªå£°
                shape = (modified_states.shape[0], end_pos - start_pos, len(dimensions))
                noise = torch.normal(
                    mean=gaussian_mean, 
                    std=gaussian_std, 
                    size=shape, 
                    device=modified_states.device,
                    dtype=modified_states.dtype
                )
                modified_states[:, start_pos:end_pos, dimensions] += noise
                
            elif intervention_type == "zero":
                # å°†æŒ‡å®šç»´åº¦ç½®é›¶
                modified_states[:, start_pos:end_pos, dimensions] = 0.0
                
            elif intervention_type == "constant":
                # å°†æŒ‡å®šç»´åº¦è®¾ä¸ºå¸¸æ•°
                modified_states[:, start_pos:end_pos, dimensions] = intervention_value
                
            elif intervention_type == "scale":
                # ç¼©æ”¾æŒ‡å®šç»´åº¦
                modified_states[:, start_pos:end_pos, dimensions] *= scale_factor
                
            elif intervention_type == "noise":
                # æ·»åŠ å™ªå£°ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                noise = torch.randn_like(modified_states[:, start_pos:end_pos, dimensions]) * intervention_value
                modified_states[:, start_pos:end_pos, dimensions] += noise
                
            elif intervention_type == "invert":
                # åè½¬æ¿€æ´»å€¼
                modified_states[:, start_pos:end_pos, dimensions] = -modified_states[:, start_pos:end_pos, dimensions]
            
            return modified_states
        
        # æ³¨å†Œå¹²é¢„é’©å­
        self.register_intervention_hook(layer_idx, intervention_fn)
        
        # è®°å½•å¹²é¢„é…ç½®
        self.interventions[layer_idx] = {
            'dimensions': dimensions,
            'type': intervention_type,
            'value': intervention_value,
            'scale_factor': scale_factor,
            'gaussian_mean': gaussian_mean,
            'gaussian_std': gaussian_std,
        }
    
    def generate_with_dual_mode_intervention(self, prompt: str, 
                                            # é€šç”¨å‚æ•°
                                            max_new_tokens: int = 32768,
                                            target_layer: int = None, 
                                            target_dimensions: List[int] = None,
                                            # NoThinkæ¨¡å¼å‚æ•°
                                            nothink_temperature: float = 0.7,
                                            nothink_top_k: int = 20,
                                            nothink_top_p: float = 0.8,
                                            nothink_do_sample: bool = True,
                                            # Thinkæ¨¡å¼å‚æ•°
                                            think_temperature: float = 0.6,
                                            think_top_k: int = 20,
                                            think_top_p: float = 0.95,
                                            think_do_sample: bool = True) -> Dict:
        """
        åŒæ—¶è¿›è¡Œthinkå’Œnothinkæ¨¡å¼çš„å¹²é¢„ç”Ÿæˆï¼Œå¹¶ç»Ÿè®¡æ¿€æ´»å€¼ï¼Œæ”¯æŒä¸ºä¸¤ç§æ¨¡å¼è®¾ç½®ä¸åŒçš„ç”Ÿæˆå‚æ•°
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆä¸¤ç§æ¨¡å¼å…±ç”¨ï¼‰
            target_layer: ç›®æ ‡å±‚ï¼ˆç”¨äºç»Ÿè®¡æ¿€æ´»å€¼ï¼‰
            target_dimensions: ç›®æ ‡ç»´åº¦ï¼ˆç”¨äºç»Ÿè®¡æ¿€æ´»å€¼ï¼‰
            
            # NoThinkæ¨¡å¼å‚æ•°
            nothink_temperature: NoThinkæ¨¡å¼çš„æ¸©åº¦å‚æ•°
            nothink_top_k: NoThinkæ¨¡å¼çš„top-k samplingå‚æ•°
            nothink_top_p: NoThinkæ¨¡å¼çš„nucleus samplingå‚æ•°
            nothink_do_sample: NoThinkæ¨¡å¼æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            
            # Thinkæ¨¡å¼å‚æ•°  
            think_temperature: Thinkæ¨¡å¼çš„æ¸©åº¦å‚æ•°
            think_top_k: Thinkæ¨¡å¼çš„top-k samplingå‚æ•°
            think_top_p: Thinkæ¨¡å¼çš„nucleus samplingå‚æ•°
            think_do_sample: Thinkæ¨¡å¼æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        
        Returns:
            åŒ…å«ä¸¤ç§æ¨¡å¼ä¸‹åŸå§‹è¾“å‡ºã€å¹²é¢„åè¾“å‡ºå’Œæ¿€æ´»å€¼ç»Ÿè®¡çš„å­—å…¸
        """
        results = {}
        
        # è·å–</think>æ ‡è®°çš„ID
        think_token_id = self._get_think_token_id()
        
        # å®šä¹‰ä¸¤ç§æ¨¡å¼çš„é…ç½®
        mode_configs = {
            "nothink": {
                "think_mode": False,
                "generation_kwargs": {
                    "max_new_tokens": max_new_tokens,
                    "temperature": nothink_temperature,
                    "top_k": nothink_top_k if nothink_top_k is not None else None,
                    "top_p": nothink_top_p,
                    "do_sample": nothink_do_sample,
                    "pad_token_id": self.tokenizer.eos_token_id,
                }
            },
            "think": {
                "think_mode": True,
                "generation_kwargs": {
                    "max_new_tokens": max_new_tokens,
                    "temperature": think_temperature,
                    "top_k": think_top_k if think_top_k is not None else None,
                    "top_p": think_top_p,
                    "do_sample": think_do_sample,
                    "pad_token_id": self.tokenizer.eos_token_id,
                }
            }
        }
        
        # ä¸ºä¸¤ç§æ¨¡å¼è¿›è¡Œå®éªŒ
        for mode_name, config in mode_configs.items():
            think_mode = config["think_mode"]
            generation_kwargs = {k: v for k, v in config["generation_kwargs"].items() if v is not None}
            
            print(f"\nå¤„ç†{mode_name}æ¨¡å¼...")
            print(f"  ç”Ÿæˆå‚æ•°: temperature={generation_kwargs.get('temperature')}, "
                  f"top_k={generation_kwargs.get('top_k')}, "
                  f"top_p={generation_kwargs.get('top_p')}, "
                  f"do_sample={generation_kwargs.get('do_sample')}")
            
            # ä½¿ç”¨å¯¹åº”æ¨¡å¼çš„æ¨¡æ¿
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True,
                enable_thinking=think_mode
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt", padding=True).to(self.device)
            
            # 1. åŸå§‹ç”Ÿæˆï¼ˆç”¨äºç»Ÿè®¡æ¿€æ´»å€¼ï¼‰
            print(f"  {mode_name}æ¨¡å¼åŸå§‹ç”Ÿæˆ...")
            self.clear_hooks()
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡å±‚å’Œç»´åº¦ï¼Œæ³¨å†Œæ¿€æ´»å€¼é’©å­è¿›è¡Œç»Ÿè®¡
            activation_stats = None
            if target_layer is not None and target_dimensions is not None:
                # ä¸ºæ¯ä¸ªç»´åº¦å•ç‹¬å­˜å‚¨æ¿€æ´»å€¼
                activation_values_by_dim = {dim: [] for dim in target_dimensions}
                hook_call_count = 0  # æ·»åŠ é’©å­è°ƒç”¨è®¡æ•°
                
                def stats_hook_fn(module, input, output):
                    nonlocal hook_call_count
                    hook_call_count += 1
                    
                    # è·å–ç›®æ ‡ç»´åº¦çš„æ¿€æ´»å€¼
                    hidden_states = output[0]
                    seq_len = hidden_states.shape[1]
                    
                    for dim_idx, dim in enumerate(target_dimensions):
                        # å¯¹äºseq_len=1çš„æƒ…å†µï¼Œå–ç¬¬0ä¸ªä½ç½®ï¼›å¯¹äºseq_len>1çš„æƒ…å†µï¼Œå–æœ€åä¸€ä¸ªä½ç½®
                        if seq_len == 1:
                            token_activation = hidden_states[:, 0, dim]
                        else:
                            token_activation = hidden_states[:, -1, dim]  # å–æœ€åä¸€ä¸ªä½ç½®
                        
                        activation_value = token_activation.flatten().detach().cpu().numpy()[0]
                        activation_values_by_dim[dim].append(activation_value)
                    
                    total_collected = sum(len(values) for values in activation_values_by_dim.values())
                
                # æ³¨å†Œç»Ÿè®¡é’©å­
                layer = self.model.model.layers[target_layer]
                stats_hook = layer.register_forward_hook(stats_hook_fn)
                print(f"  {mode_name}æ¨¡å¼: å·²æ³¨å†Œæ¿€æ´»å€¼ç»Ÿè®¡é’©å­åœ¨ç¬¬{target_layer}å±‚")
            
            with torch.no_grad():
                original_outputs = self.model.generate(**inputs, **generation_kwargs)
            
            # è®¡ç®—æ¿€æ´»å€¼ç»Ÿè®¡
            if target_layer is not None and target_dimensions is not None:
                stats_hook.remove()
                
                # è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                original_output_ids = original_outputs[0][len(inputs.input_ids[0]):].tolist()
                original_text = self.tokenizer.decode(original_outputs[0], skip_special_tokens=True)
                generated_text = self.tokenizer.decode(original_output_ids, skip_special_tokens=False)
                
                total_collected = sum(len(values) for values in activation_values_by_dim.values())
                
                print(f"  {mode_name}æ¨¡å¼è°ƒè¯•ä¿¡æ¯:")
                print(f"    é’©å­æ€»è°ƒç”¨æ¬¡æ•°: {hook_call_count}")
                print(f"    æ”¶é›†åˆ°çš„æ¿€æ´»å€¼æ€»æ•°: {total_collected}")
                print(f"    ç›®æ ‡ç»´åº¦æ•°é‡: {len(target_dimensions)}")
                print(f"    æ¯ä¸ªç»´åº¦æ”¶é›†çš„tokenæ•°: {len(list(activation_values_by_dim.values())[0]) if activation_values_by_dim else 0}")
                print(f"    å®é™…ç”Ÿæˆçš„token IDæ•°é‡: {len(original_output_ids)}")
                print(f"    ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦: {len(generated_text)}")
                
                # ä¸ºæ¯ä¸ªç»´åº¦åˆ†åˆ«ç»Ÿè®¡
                for dim in target_dimensions:
                    values = activation_values_by_dim[dim]
                    if values:
                        print(f"    ç»´åº¦ {dim} ç»Ÿè®¡: count={len(values)}, mean={np.mean(values):.6f}, std={np.std(values):.6f}")
                
                if activation_values_by_dim and any(values for values in activation_values_by_dim.values()):
                    activation_stats = {
                        'by_dimension': {
                            dim: {
                                'mean': float(np.mean(values)) if values else 0.0,
                                'std': float(np.std(values)) if values else 0.0,
                                'variance': float(np.var(values)) if values else 0.0,
                                'min': float(np.min(values)) if values else 0.0,
                                'max': float(np.max(values)) if values else 0.0,
                                'count': len(values)
                            }
                            for dim, values in activation_values_by_dim.items()
                        },
                        'overall': {
                            'total_collected': total_collected,
                            'tokens_per_dimension': len(list(activation_values_by_dim.values())[0]) if activation_values_by_dim else 0,
                            'actual_generated_tokens': len(original_output_ids)
                        }
                    }
                    print(f"  {mode_name}æ¨¡å¼æ¿€æ´»å€¼ç»Ÿè®¡å®Œæˆ")
                else:
                    print(f"  {mode_name}æ¨¡å¼: æœªæ”¶é›†åˆ°æ¿€æ´»å€¼")
                    activation_stats = None
            
            # æå–ç”Ÿæˆçš„å†…å®¹
            original_output_ids = original_outputs[0][len(inputs.input_ids[0]):].tolist()
            original_text = self.tokenizer.decode(original_outputs[0], skip_special_tokens=True)
            
            # åˆ†ç¦»thinkingå’Œsolution
            original_thinking, original_solution = self._separate_thinking_and_solution(
                original_output_ids, think_token_id, think_mode
            )
            
            # 2. å¹²é¢„ç”Ÿæˆ
            print(f"  {mode_name}æ¨¡å¼å¹²é¢„ç”Ÿæˆ...")
            
            # é‡æ–°æ³¨å†Œå¹²é¢„é’©å­
            for layer_idx, config_item in self.interventions.items():
                self.set_dimension_intervention(
                    layer_idx, 
                    config_item['dimensions'], 
                    config_item['type'],
                    config_item['value'],
                    config_item['scale_factor'],
                    config_item.get('gaussian_mean', 0.0),
                    config_item.get('gaussian_std', 1.0)
                )
            
            with torch.no_grad():
                intervention_outputs = self.model.generate(**inputs, **generation_kwargs)
            
            # æå–å¹²é¢„åçš„ç”Ÿæˆå†…å®¹
            intervention_output_ids = intervention_outputs[0][len(inputs.input_ids[0]):].tolist()
            intervention_text = self.tokenizer.decode(intervention_outputs[0], skip_special_tokens=True)
            
            # åˆ†ç¦»thinkingå’Œsolution
            intervention_thinking, intervention_solution = self._separate_thinking_and_solution(
                intervention_output_ids, think_token_id, think_mode
            )
            
            # ä¿å­˜è¯¥æ¨¡å¼çš„ç»“æœ
            results[mode_name] = {
                'formatted_prompt': formatted_prompt,
                'original_text': original_text,
                'intervention_text': intervention_text,
                'original_thinking': original_thinking,
                'original_solution': original_solution,
                'intervention_thinking': intervention_thinking,
                'intervention_solution': intervention_solution,
                'activation_stats': activation_stats,
                'generation_config': generation_kwargs
            }
        
        # è¿”å›å®Œæ•´ç»“æœ
        return {
            'prompt': prompt,
            'nothink_mode': results['nothink'],
            'think_mode': results['think'],
            'interventions': copy.deepcopy(self.interventions),
            'generation_configs': {
                'nothink': mode_configs['nothink']['generation_kwargs'],
                'think': mode_configs['think']['generation_kwargs']
            },
            'activation_analysis': {
                'nothink_mode': results['nothink']['activation_stats'],
                'think_mode': results['think']['activation_stats']
            }
        }
    
    def _get_think_token_id(self):
        """è·å–</think>çš„token ID"""
        try:
            think_token = "</think>"
            think_token_id = self.tokenizer.convert_tokens_to_ids(think_token)
            if think_token_id == self.tokenizer.unk_token_id:
                # å¦‚æœè½¬æ¢ç»“æœæ˜¯unk tokenï¼Œå°è¯•å…¶ä»–æ–¹å¼
                think_token_id = None
                
                # æ–¹æ³•1: å°è¯•ç¼–ç æ•´ä¸ªæ ‡è®°å¹¶è·å–æœ€åä¸€ä¸ªtoken
                tokens = self.tokenizer.tokenize(think_token)
                if tokens and tokens[-1] != self.tokenizer.unk_token:
                    think_token_id = self.tokenizer.convert_tokens_to_ids(tokens[-1])
                
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                if think_token_id is None or think_token_id == self.tokenizer.unk_token_id:
                    # Qwen3çš„</think>æ ‡è®°IDå¯èƒ½æ˜¯151668
                    print("æ— æ³•ç¡®å®š</think>æ ‡è®°IDï¼Œä½¿ç”¨é»˜è®¤å€¼151668")
                    think_token_id = 151668
            
            return think_token_id
        except Exception as e:
            print(f"è·å–</think>æ ‡è®°IDå¤±è´¥: {e}")
            return 151668  # é»˜è®¤å€¼
    
    def _separate_thinking_and_solution(self, output_ids, think_token_id, think_mode):
        """åˆ†ç¦»thinkingå†…å®¹å’Œsolutionå†…å®¹"""
        if not think_mode:
            # nothinkæ¨¡å¼ï¼Œæ•´ä¸ªè¾“å‡ºéƒ½æ˜¯solution
            solution = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
            return "", solution
        
        try:
            # å¯»æ‰¾</think>æ ‡è®°çš„ä½ç½®
            think_index = len(output_ids) - output_ids[::-1].index(think_token_id)
        except ValueError:
            # æœªæ‰¾åˆ°</think>æ ‡è®°ï¼Œå°†å®Œæ•´è¾“å‡ºä½œä¸ºsolution
            solution = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
            return "", solution
        
        # åˆ†ç¦»thinkingå’Œsolution
        thinking_content = self.tokenizer.decode(output_ids[:think_index], skip_special_tokens=True).strip()
        solution_content = self.tokenizer.decode(output_ids[think_index:], skip_special_tokens=True).strip()
        
        return thinking_content, solution_content

def run_intervention_experiment(model_path: str, 
                              prompts: List[str],
                              target_layers: Union[int, List[int]],  # æ”¯æŒå•å±‚æˆ–å¤šå±‚
                              target_dimensions: List[int],
                              intervention_types: List[str] = ["gaussian_replace"],
                              scale_factors: List[float] = [0.5, 2.0],
                              gaussian_params: List[Dict] = None,
                              output_dir: str = "intervention_results",
                              # é€šç”¨å‚æ•°
                              max_new_tokens: int = 32768,
                              # NoThinkæ¨¡å¼å‚æ•°
                              nothink_temperature: float = 0.7,
                              nothink_top_k: int = 20,
                              nothink_top_p: float = 0.8,
                              nothink_do_sample: bool = True,
                              # Thinkæ¨¡å¼å‚æ•°
                              think_temperature: float = 0.6,
                              think_top_k: int = 20,
                              think_top_p: float = 0.95,
                              think_do_sample: bool = True,
                              device: str = "cuda",
                              gpu_id: int = 0):
    """
    è¿è¡Œå¹²é¢„å®éªŒï¼Œæ”¯æŒä¸ºthinkå’Œnothinkæ¨¡å¼è®¾ç½®ä¸åŒçš„ç”Ÿæˆå‚æ•°ï¼Œæ”¯æŒå¤šå±‚åŒæ—¶å¹²é¢„
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
        target_layers: ç›®æ ‡å±‚(å•ä¸ªå±‚ç´¢å¼•æˆ–å±‚ç´¢å¼•åˆ—è¡¨)
        target_dimensions: ç›®æ ‡ç»´åº¦åˆ—è¡¨
        intervention_types: å¹²é¢„ç±»å‹åˆ—è¡¨
        scale_factors: ç¼©æ”¾å› å­åˆ—è¡¨ï¼ˆä»…å¯¹scaleç±»å‹æœ‰æ•ˆï¼‰
        gaussian_params: é«˜æ–¯åˆ†å¸ƒå‚æ•°åˆ—è¡¨ [{"mean": 0.0, "std": 1.0}, ...]
        output_dir: è¾“å‡ºç›®å½•
        
        # é€šç”¨å‚æ•°
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆä¸¤ç§æ¨¡å¼å…±ç”¨ï¼‰
        
        # NoThinkæ¨¡å¼å‚æ•°
        nothink_temperature: NoThinkæ¨¡å¼çš„æ¸©åº¦å‚æ•°
        nothink_top_k: NoThinkæ¨¡å¼çš„top-k samplingå‚æ•°
        nothink_top_p: NoThinkæ¨¡å¼çš„nucleus samplingå‚æ•°
        nothink_do_sample: NoThinkæ¨¡å¼æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        
        # Thinkæ¨¡å¼å‚æ•°
        think_temperature: Thinkæ¨¡å¼çš„æ¸©åº¦å‚æ•°
        think_top_k: Thinkæ¨¡å¼çš„top-k samplingå‚æ•°
        think_top_p: Thinkæ¨¡å¼çš„nucleus samplingå‚æ•°
        think_do_sample: Thinkæ¨¡å¼æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        
        device: è®¾å¤‡ç±»å‹
        gpu_id: GPUè®¾å¤‡ID
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # å¤„ç†å•å±‚å’Œå¤šå±‚è¾“å…¥
    if isinstance(target_layers, int):
        target_layers = [target_layers]
    
    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
    print(f"ç›®æ ‡å±‚: {target_layers}")
    
    # é»˜è®¤é«˜æ–¯å‚æ•°
    if gaussian_params is None:
        gaussian_params = [
            {"mean": 0, "std": 100}
        ]
    
    # åˆå§‹åŒ–æ§åˆ¶å™¨
    controller = NeuralInterventionController(model_path, device, gpu_id)
    
    all_results = []
    
    print(f"åŒæ¨¡å¼ç”Ÿæˆé…ç½®:")
    print(f"  NoThinkæ¨¡å¼: temperature={nothink_temperature}, top_k={nothink_top_k}, top_p={nothink_top_p}, do_sample={nothink_do_sample}")
    print(f"  Thinkæ¨¡å¼: temperature={think_temperature}, top_k={think_top_k}, top_p={think_top_p}, do_sample={think_do_sample}")
    print(f"  æœ€å¤§ç”Ÿæˆtokenæ•°: {max_new_tokens}")
    
    for prompt_idx, prompt in enumerate(prompts):
        print(f"\nå¤„ç†æç¤º {prompt_idx + 1}/{len(prompts)}: {prompt[:50]}...")
        
        for intervention_type in intervention_types:
            if intervention_type == "scale":
                test_values = scale_factors
                param_type = "scale"
            elif intervention_type in ["gaussian_replace", "gaussian_noise"]:
                test_values = gaussian_params
                param_type = "gaussian"
            else:
                test_values = [0.0]  # å¯¹äºå…¶ä»–ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼
                param_type = "default"
            
            for value in test_values:
                if param_type == "gaussian":
                    print(f"  å¹²é¢„ç±»å‹: {intervention_type}, é«˜æ–¯å‚æ•°: mean={value['mean']}, std={value['std']}")
                else:
                    print(f"  å¹²é¢„ç±»å‹: {intervention_type}, å€¼: {value}")
                
                # æ¸…é™¤ä¹‹å‰çš„å¹²é¢„
                controller.clear_hooks()
                controller.interventions.clear()
                
                print(f"  ğŸ¯ è®¾ç½®å¹²é¢„: å±‚{target_layers}, ç»´åº¦{len(target_dimensions)}ä¸ª")
                
                # è®¾ç½®æ–°çš„å¹²é¢„ - æ”¯æŒå¤šå±‚
                for target_layer in target_layers:
                    if intervention_type == "scale":
                        controller.set_dimension_intervention(
                            target_layer, target_dimensions, intervention_type, 
                            scale_factor=value,
                        )
                    elif intervention_type in ["gaussian_replace", "gaussian_noise"]:
                        controller.set_dimension_intervention(
                            target_layer, target_dimensions, intervention_type,
                            gaussian_mean=value["mean"],
                            gaussian_std=value["std"],
                        )
                    else:
                        controller.set_dimension_intervention(
                            target_layer, target_dimensions, intervention_type, 
                            intervention_value=value,
                        )
                
                print(f"  âœ… å¹²é¢„è®¾ç½®å®Œæˆ: {len(controller.hooks)}ä¸ªhookå·²æ³¨å†Œ")
                
                # ç”Ÿæˆç»“æœï¼Œä¼ é€’åˆ†åˆ«çš„æ¨¡å¼å‚æ•°
                # æ³¨æ„ï¼šå¯¹äºå¤šå±‚å¹²é¢„ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬ä¸€å±‚è¿›è¡Œæ¿€æ´»å€¼ç»Ÿè®¡
                result = controller.generate_with_dual_mode_intervention(
                    prompt, 
                    max_new_tokens=max_new_tokens,
                    target_layer=target_layers[0],  # ä½¿ç”¨ç¬¬ä¸€å±‚è¿›è¡Œç»Ÿè®¡
                    target_dimensions=target_dimensions,
                    nothink_temperature=nothink_temperature,
                    nothink_top_k=nothink_top_k,
                    nothink_top_p=nothink_top_p,
                    nothink_do_sample=nothink_do_sample,
                    think_temperature=think_temperature,
                    think_top_k=think_top_k,
                    think_top_p=think_top_p,
                    think_do_sample=think_do_sample
                )
                result['prompt_idx'] = prompt_idx
                if param_type == "gaussian":
                    result['intervention_config'] = {
                        'layers': target_layers,  # æ›´æ–°ä¸ºlayersåˆ—è¡¨
                        'dimensions': target_dimensions,
                        'type': intervention_type,
                        'gaussian_mean': value["mean"],
                        'gaussian_std': value["std"]
                    }
                else:
                    result['intervention_config'] = {
                        'layers': target_layers,  # æ›´æ–°ä¸ºlayersåˆ—è¡¨
                        'dimensions': target_dimensions,
                        'type': intervention_type,
                        'value': value
                    }
                
                all_results.append(result)
                
    
    # ä¿å­˜ç»“æœ
    # ä¸ºäº†é¿å…æ–‡ä»¶åè¿‡é•¿ï¼Œä½¿ç”¨ç»´åº¦æ•°é‡å’Œå“ˆå¸Œå€¼
    dims_hash = hashlib.md5(str(target_dimensions).encode()).hexdigest()[:8]
    output_file = os.path.join(output_dir, f"dual_mode_intervention_layer_{target_layers[0]}_dims_{len(target_dimensions)}dims_{dims_hash}.json")
    
    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶çš„ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # è½¬æ¢tensorä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_results = []
    for result in all_results:
        serializable_result = copy.deepcopy(result)
        # æ¿€æ´»å€¼ç»Ÿè®¡å·²ç»æ˜¯åŸºæœ¬æ•°æ®ç±»å‹ï¼Œå¯ä»¥ç›´æ¥åºåˆ—åŒ–ï¼Œä¸éœ€è¦åˆ é™¤
        # åªç§»é™¤å¯èƒ½å­˜åœ¨çš„torch tensorå¯¹è±¡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        serializable_results.append(serializable_result)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”ŸæˆåŒæ¨¡å¼åˆ†ææŠ¥å‘Š
    generate_dual_mode_analysis_report(all_results, output_dir, target_layers[0], target_dimensions)
    
    return all_results


def generate_dual_mode_analysis_report(results: List[Dict], output_dir: str, 
                           target_layer: int, target_dimensions: List[int]):
    """ç”ŸæˆåŒæ¨¡å¼åˆ†ææŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰"""
    dims_hash = hashlib.md5(str(target_dimensions).encode()).hexdigest()[:8]
    report_file = os.path.join(output_dir, f"dual_mode_analysis_report_layer_{target_layer}_dims_{len(target_dimensions)}dims_{dims_hash}.md")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç¥ç»å¹²é¢„åŒæ¨¡å¼å®éªŒåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"## å®éªŒé…ç½®\n\n")
        f.write(f"- **ç›®æ ‡å±‚**: {target_layer}\n")
        f.write(f"- **ç›®æ ‡ç»´åº¦**: {target_dimensions}\n")
        f.write(f"- **æ€»å®éªŒæ•°**: {len(results)}\n\n")
        
        # ç»Ÿè®¡ä¸åŒå¹²é¢„ç±»å‹çš„ç»“æœ
        intervention_types = {}
        for result in results:
            intervention_type = result['intervention_config']['type']
            if intervention_type not in intervention_types:
                intervention_types[intervention_type] = []
            intervention_types[intervention_type].append(result)
        
        for intervention_type, type_results in intervention_types.items():
            f.write(f"## {intervention_type} å¹²é¢„ç±»å‹ç»“æœ\n\n")
            
            # ç»Ÿè®¡æ¿€æ´»å€¼å˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if 'activation_analysis' in type_results[0] and type_results[0]['activation_analysis']:
                think_stats_list = []
                nothink_stats_list = []
                
                for result in type_results:
                    activation_analysis = result['activation_analysis']
                    if activation_analysis.get('think_mode'):
                        think_stats_list.append(activation_analysis['think_mode'])
                    if activation_analysis.get('nothink_mode'):
                        nothink_stats_list.append(activation_analysis['nothink_mode'])
                
                if think_stats_list and nothink_stats_list:
                    # å¤„ç†æ–°çš„æ•°æ®ç»“æ„ï¼ˆåˆ†ç»´åº¦ç»Ÿè®¡ï¼‰
                    f.write(f"### æ¿€æ´»å€¼ç»Ÿè®¡\n\n")
                    
                    # å¦‚æœæœ‰åˆ†ç»´åº¦ç»Ÿè®¡
                    if (think_stats_list[0] and 'by_dimension' in think_stats_list[0] and
                        nothink_stats_list[0] and 'by_dimension' in nothink_stats_list[0]):
                        
                        # è·å–æ‰€æœ‰ç»´åº¦
                        dimensions = list(think_stats_list[0]['by_dimension'].keys())
                        
                        # åˆ›å»ºè¡¨æ ¼è¡¨å¤´
                        f.write("| ç»´åº¦ | æ¨¡å¼ | å¹³å‡å‡å€¼ | å¹³å‡æ–¹å·® | å¹³å‡æ ·æœ¬æ•° |\n")
                        f.write("|------|------|----------|----------|------------|\n")
                        
                        for dim in dimensions:
                            # è®¡ç®—æ¯ä¸ªç»´åº¦çš„å¹³å‡ç»Ÿè®¡
                            think_means = [stats['by_dimension'][dim]['mean'] for stats in think_stats_list 
                                         if stats and 'by_dimension' in stats and dim in stats['by_dimension']]
                            think_vars = [stats['by_dimension'][dim]['variance'] for stats in think_stats_list 
                                        if stats and 'by_dimension' in stats and dim in stats['by_dimension']]
                            think_counts = [stats['by_dimension'][dim]['count'] for stats in think_stats_list 
                                          if stats and 'by_dimension' in stats and dim in stats['by_dimension']]
                            
                            nothink_means = [stats['by_dimension'][dim]['mean'] for stats in nothink_stats_list 
                                           if stats and 'by_dimension' in stats and dim in stats['by_dimension']]
                            nothink_vars = [stats['by_dimension'][dim]['variance'] for stats in nothink_stats_list 
                                          if stats and 'by_dimension' in stats and dim in stats['by_dimension']]
                            nothink_counts = [stats['by_dimension'][dim]['count'] for stats in nothink_stats_list 
                                            if stats and 'by_dimension' in stats and dim in stats['by_dimension']]
                            
                            if think_means and nothink_means:
                                avg_think_mean = sum(think_means) / len(think_means)
                                avg_think_var = sum(think_vars) / len(think_vars)
                                avg_think_count = sum(think_counts) / len(think_counts)
                                
                                avg_nothink_mean = sum(nothink_means) / len(nothink_means)
                                avg_nothink_var = sum(nothink_vars) / len(nothink_vars)
                                avg_nothink_count = sum(nothink_counts) / len(nothink_counts)

                                f.write(f"| {dim} | Thinkæ¨¡å¼ | {avg_think_mean:.4f} | {avg_think_var:.4f} | {avg_think_count:.0f} |\n")
                                f.write(f"| {dim} | NoThinkæ¨¡å¼ | {avg_nothink_mean:.4f} | {avg_nothink_var:.4f} | {avg_nothink_count:.0f} |\n")
                        f.write("\n")
                    else:
                        f.write("**æ¿€æ´»å€¼ç»Ÿè®¡**: æ•°æ®ä¸å®Œæ•´\n\n")
                else:
                    f.write("**æ¿€æ´»å€¼ç»Ÿè®¡**: æœªæ”¶é›†åˆ°å®Œæ•´æ•°æ®\n\n")
            else:
                f.write("**æ¿€æ´»å€¼ç»Ÿè®¡**: æœªå¯ç”¨ç»Ÿè®¡åŠŸèƒ½\n\n")
            
            # æ˜¾ç¤ºå‰3ä¸ªä¾‹å­
            f.write("### å®éªŒç¤ºä¾‹\n\n")
            for i, result in enumerate(type_results[:3]):
                f.write(f"#### ä¾‹å­ {i+1}\n\n")
                f.write(f"**æç¤º**: {result['prompt']}\n\n")
                f.write(f"**å¹²é¢„é…ç½®**: {result['intervention_config']}\n\n")
                
                # NoThinkæ¨¡å¼ç»“æœ
                f.write(f"##### NoThinkæ¨¡å¼\n\n")
                f.write(f"**åŸå§‹å›ç­”** (é•¿åº¦: {len(result['nothink_mode']['original_solution'])})\n\n")
                f.write(f"{result['nothink_mode']['original_solution']}\n\n")
                f.write(f"**å¹²é¢„å›ç­”** (é•¿åº¦: {len(result['nothink_mode']['intervention_solution'])})\n\n")
                f.write(f"{result['nothink_mode']['intervention_solution']}\n\n")

                # Thinkæ¨¡å¼ç»“æœ
                f.write(f"##### Thinkæ¨¡å¼\n\n")
                f.write(f"**åŸå§‹æ€è€ƒè¿‡ç¨‹** (é•¿åº¦: {len(result['think_mode']['original_thinking'])})\n\n")
                f.write(f"{result['think_mode']['original_thinking']}\n\n")
                f.write(f"**åŸå§‹å›ç­”** (é•¿åº¦: {len(result['think_mode']['original_solution'])})\n\n")
                f.write(f"{result['think_mode']['original_solution']}\n\n")
                f.write(f"**å¹²é¢„æ€è€ƒè¿‡ç¨‹** (é•¿åº¦: {len(result['think_mode']['intervention_thinking'])})\n\n")
                f.write(f"{result['think_mode']['intervention_thinking']}\n\n")
                f.write(f"**å¹²é¢„å›ç­”** (é•¿åº¦: {len(result['think_mode']['intervention_solution'])})\n\n")
                f.write(f"{result['think_mode']['intervention_solution']}\n\n")
                
                f.write(f"---\n\n")
    
    print(f"åŒæ¨¡å¼åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def parse_args():
    parser = argparse.ArgumentParser(description='ç¥ç»å¹²é¢„å®éªŒ - æ”¯æŒåŒæ¨¡å¼ä¸åŒå‚æ•°')
    parser.add_argument('--model_path', type=str, 
                        default='/data4/huguangyi/models/Qwen/Qwen3-0.6B',
                        help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--target_layer', type=str, default='14',
                        help='ç›®æ ‡å±‚ç´¢å¼•ï¼ˆå¯ä»¥æ˜¯å•ä¸ªæ•°å­—æˆ–ç”¨é€—å·åˆ†éš”çš„å¤šä¸ªæ•°å­—ï¼Œå¦‚"14"æˆ–"12,13,14"ï¼‰')
    parser.add_argument('--target_dimensions', type=str, default='16',
                        help='ç›®æ ‡ç»´åº¦ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--intervention_types', type=str, default='zero',
                        help='å¹²é¢„ç±»å‹ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--prompts_file', type=str, default=None,
                        help='åŒ…å«æµ‹è¯•æç¤ºçš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='intervention_results',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®¾å¤‡')
    
    # GPUè®¾å¤‡å‚æ•°
    parser.add_argument('--gpu_id', type=int, default=6,
                        help='æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡IDï¼ˆé»˜è®¤ï¼š4ï¼‰')
    
    # é€šç”¨ç”Ÿæˆé…ç½®å‚æ•°
    parser.add_argument('--max_new_tokens', type=int, default=32768,
                        help='æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆä¸¤ç§æ¨¡å¼å…±ç”¨ï¼‰')
    
    # NoThinkæ¨¡å¼ç”Ÿæˆé…ç½®å‚æ•°
    parser.add_argument('--nothink_temperature', type=float, default=0.0,
                        help='NoThinkæ¨¡å¼çš„æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§')
    parser.add_argument('--nothink_top_k', type=int, default=20,
                        help='NoThinkæ¨¡å¼çš„top-k samplingå‚æ•°')
    parser.add_argument('--nothink_top_p', type=float, default=0.8,
                        help='NoThinkæ¨¡å¼çš„nucleus samplingå‚æ•°')
    parser.add_argument('--nothink_do_sample', action='store_true', default=False,
                        help='NoThinkæ¨¡å¼æ˜¯å¦ä½¿ç”¨é‡‡æ ·')
    parser.add_argument('--nothink_no_sample', dest='nothink_do_sample', action='store_false',
                        help='NoThinkæ¨¡å¼ç¦ç”¨é‡‡æ ·')
    
    # Thinkæ¨¡å¼ç”Ÿæˆé…ç½®å‚æ•°
    parser.add_argument('--think_temperature', type=float, default=0.0,
                        help='Thinkæ¨¡å¼çš„æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§')
    parser.add_argument('--think_top_k', type=int, default=20,
                        help='Thinkæ¨¡å¼çš„top-k samplingå‚æ•°')
    parser.add_argument('--think_top_p', type=float, default=0.95,
                        help='Thinkæ¨¡å¼çš„nucleus samplingå‚æ•°')
    parser.add_argument('--think_do_sample', action='store_true', default=False,
                        help='Thinkæ¨¡å¼æ˜¯å¦ä½¿ç”¨é‡‡æ ·')
    parser.add_argument('--think_no_sample', dest='think_do_sample', action='store_false',
                        help='Thinkæ¨¡å¼ç¦ç”¨é‡‡æ ·')
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    # è§£æç›®æ ‡ç»´åº¦
    target_dimensions = [int(d.strip()) for d in args.target_dimensions.split(',')]
    
    # è§£æå¹²é¢„ç±»å‹
    intervention_types = [t.strip() for t in args.intervention_types.split(',')]
    
    # è§£æç›®æ ‡å±‚
    target_layers = [int(l.strip()) for l in args.target_layer.split(',')]
    
    print(f"å°†å¯¹æ¨¡å‹ {args.model_path} è¿›è¡Œç¥ç»å¹²é¢„åŒæ¨¡å¼å®éªŒ")
    print(f"è®¾å¤‡: {args.device}")
    if args.device == "cuda":
        print(f"GPU ID: {args.gpu_id}")
    print(f"ç›®æ ‡å±‚: {target_layers}")
    print(f"ç›®æ ‡ç»´åº¦: {target_dimensions}")
    print(f"å¹²é¢„ç±»å‹: {intervention_types}")
    print(f"æœ€å¤§ç”Ÿæˆtokenæ•°: {args.max_new_tokens}")
    print(f"NoThinkæ¨¡å¼å‚æ•°: temperature={args.nothink_temperature}, top_k={args.nothink_top_k}, top_p={args.nothink_top_p}, do_sample={args.nothink_do_sample}")
    print(f"Thinkæ¨¡å¼å‚æ•°: temperature={args.think_temperature}, top_k={args.think_top_k}, top_p={args.think_top_p}, do_sample={args.think_do_sample}")
    
    # å‡†å¤‡æµ‹è¯•æç¤º
    if args.prompts_file and os.path.exists(args.prompts_file):
        with open(args.prompts_file, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]
    else:
        # é»˜è®¤æµ‹è¯•æç¤º
        prompts = [
            "Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$"
        ]
    
    # è¿è¡Œå®éªŒ
    results = run_intervention_experiment(
        model_path=args.model_path,
        prompts=prompts,
        target_layers=target_layers,
        target_dimensions=target_dimensions,
        intervention_types=intervention_types,
        output_dir=args.output_dir,
        max_new_tokens=args.max_new_tokens,
        nothink_temperature=args.nothink_temperature,
        nothink_top_k=args.nothink_top_k,
        nothink_top_p=args.nothink_top_p,
        nothink_do_sample=args.nothink_do_sample,
        think_temperature=args.think_temperature,
        think_top_k=args.think_top_k,
        think_top_p=args.think_top_p,
        think_do_sample=args.think_do_sample,
        device=args.device,
        gpu_id=args.gpu_id
    )
    
    print(f"\nå®éªŒå®Œæˆï¼å…±è¿›è¡Œäº† {len(results)} æ¬¡å¹²é¢„æµ‹è¯•ã€‚")


if __name__ == "__main__":
    main() 